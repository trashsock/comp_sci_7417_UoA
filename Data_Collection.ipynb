{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "NM7s12bTPUvk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "import os\n",
        "from openpyxl import load_workbook\n"
      ],
      "metadata": {
        "id": "5HIFXK0GPSyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# First Data Collection Run (Failed due to the API quota getting over)"
      ],
      "metadata": {
        "id": "QuHKxgxTPyFr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blURrkKtS-wS",
        "outputId": "ffe597da-2d26-4afe-de96-33a1b53a8e66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collected page 1, total posts: 100\n",
            "Saved page 1 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 2, total posts: 200\n",
            "Saved page 2 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 3, total posts: 300\n",
            "Saved page 3 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 4, total posts: 400\n",
            "Saved page 4 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 5, total posts: 500\n",
            "Saved page 5 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 6, total posts: 600\n",
            "Saved page 6 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 7, total posts: 700\n",
            "Saved page 7 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 8, total posts: 800\n",
            "Saved page 8 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 9, total posts: 900\n",
            "Saved page 9 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 10, total posts: 1000\n",
            "Saved page 10 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 11, total posts: 1100\n",
            "Saved page 11 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 12, total posts: 1200\n",
            "Saved page 12 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 13, total posts: 1300\n",
            "Saved page 13 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 14, total posts: 1400\n",
            "Saved page 14 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 15, total posts: 1500\n",
            "Saved page 15 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 16, total posts: 1600\n",
            "Saved page 16 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 17, total posts: 1700\n",
            "Saved page 17 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 18, total posts: 1800\n",
            "Saved page 18 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 19, total posts: 1900\n",
            "Saved page 19 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 20, total posts: 2000\n",
            "Saved page 20 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 21, total posts: 2100\n",
            "Saved page 21 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 22, total posts: 2200\n",
            "Saved page 22 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 23, total posts: 2300\n",
            "Saved page 23 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 24, total posts: 2400\n",
            "Saved page 24 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 25, total posts: 2500\n",
            "Saved page 25 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 26, total posts: 2600\n",
            "Saved page 26 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 27, total posts: 2700\n",
            "Saved page 27 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 28, total posts: 2800\n",
            "Saved page 28 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 29, total posts: 2900\n",
            "Saved page 29 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 30, total posts: 3000\n",
            "Saved page 30 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 31, total posts: 3100\n",
            "Saved page 31 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 32, total posts: 3200\n",
            "Saved page 32 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 33, total posts: 3300\n",
            "Saved page 33 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 34, total posts: 3400\n",
            "Saved page 34 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 35, total posts: 3500\n",
            "Saved page 35 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 36, total posts: 3600\n",
            "Saved page 36 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 37, total posts: 3700\n",
            "Saved page 37 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 38, total posts: 3800\n",
            "Saved page 38 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 39, total posts: 3900\n",
            "Saved page 39 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 40, total posts: 4000\n",
            "Saved page 40 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 41, total posts: 4100\n",
            "Saved page 41 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 42, total posts: 4200\n",
            "Saved page 42 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 43, total posts: 4300\n",
            "Saved page 43 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 44, total posts: 4400\n",
            "Saved page 44 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 45, total posts: 4500\n",
            "Saved page 45 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 46, total posts: 4600\n",
            "Saved page 46 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 47, total posts: 4700\n",
            "Saved page 47 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 48, total posts: 4800\n",
            "Saved page 48 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 49, total posts: 4900\n",
            "Saved page 49 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 50, total posts: 5000\n",
            "Saved page 50 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 51, total posts: 5100\n",
            "Saved page 51 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 52, total posts: 5200\n",
            "Saved page 52 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 53, total posts: 5300\n",
            "Saved page 53 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 54, total posts: 5400\n",
            "Saved page 54 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 55, total posts: 5500\n",
            "Saved page 55 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 56, total posts: 5600\n",
            "Saved page 56 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 57, total posts: 5700\n",
            "Saved page 57 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 58, total posts: 5800\n",
            "Saved page 58 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 59, total posts: 5900\n",
            "Saved page 59 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 60, total posts: 6000\n",
            "Saved page 60 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 61, total posts: 6100\n",
            "Saved page 61 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 62, total posts: 6200\n",
            "Saved page 62 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 63, total posts: 6300\n",
            "Saved page 63 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 64, total posts: 6400\n",
            "Saved page 64 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 65, total posts: 6500\n",
            "Saved page 65 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 66, total posts: 6600\n",
            "Saved page 66 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 67, total posts: 6700\n",
            "Saved page 67 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 68, total posts: 6800\n",
            "Saved page 68 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 69, total posts: 6900\n",
            "Saved page 69 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 70, total posts: 7000\n",
            "Saved page 70 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 71, total posts: 7100\n",
            "Saved page 71 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 72, total posts: 7200\n",
            "Saved page 72 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 73, total posts: 7300\n",
            "Saved page 73 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 74, total posts: 7400\n",
            "Saved page 74 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 75, total posts: 7500\n",
            "Saved page 75 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 76, total posts: 7600\n",
            "Saved page 76 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 77, total posts: 7700\n",
            "Saved page 77 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 78, total posts: 7800\n",
            "Saved page 78 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 79, total posts: 7900\n",
            "Saved page 79 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 80, total posts: 8000\n",
            "Saved page 80 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 81, total posts: 8100\n",
            "Saved page 81 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 82, total posts: 8200\n",
            "Saved page 82 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 83, total posts: 8300\n",
            "Saved page 83 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 84, total posts: 8400\n",
            "Saved page 84 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 85, total posts: 8500\n",
            "Saved page 85 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 86, total posts: 8600\n",
            "Saved page 86 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 87, total posts: 8700\n",
            "Saved page 87 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 88, total posts: 8800\n",
            "Saved page 88 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 89, total posts: 8900\n",
            "Saved page 89 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 90, total posts: 9000\n",
            "Saved page 90 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 91, total posts: 9100\n",
            "Saved page 91 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 92, total posts: 9200\n",
            "Saved page 92 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 93, total posts: 9300\n",
            "Saved page 93 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 94, total posts: 9400\n",
            "Saved page 94 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 95, total posts: 9500\n",
            "Saved page 95 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 96, total posts: 9600\n",
            "Saved page 96 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 97, total posts: 9700\n",
            "Saved page 97 to nlp_stackoverflow_dataset.xlsx\n",
            "Collected page 98, total posts: 9800\n",
            "Saved page 98 to nlp_stackoverflow_dataset.xlsx\n",
            "Error fetching answers for post 57468725: {'error_id': 502, 'error_message': 'too many requests from this IP, more requests available in 84488 seconds', 'error_name': 'throttle_violation'}\n",
            "Collected page 99, total posts: 9806\n",
            "Saved page 99 to nlp_stackoverflow_dataset.xlsx\n",
            "Quota nearly exhausted (0 remaining), moving to preprocessing...\n",
            "Collected 9806 posts, saved to nlp_stackoverflow_dataset.xlsx\n"
          ]
        }
      ],
      "source": [
        "\n",
        "API_KEY = \"rl_iqe6KXFPAWpJYip5t6ZyHpq6H\"  # Your API key\n",
        "BASE_URL = \"https://api.stackexchange.com/2.3/questions\"\n",
        "EXCEL_FILE = \"nlp_stackoverflow_dataset.xlsx\"\n",
        "posts = []\n",
        "\n",
        "# Load existing data if file exists\n",
        "if os.path.exists(EXCEL_FILE):\n",
        "    existing_df = pd.read_excel(EXCEL_FILE, sheet_name=\"Raw_Data\")\n",
        "    posts = existing_df.to_dict(\"records\")\n",
        "    start_page = (len(posts) // 100) + 1\n",
        "    print(f\"Resuming from page {start_page}, {len(posts)} posts already collected\")\n",
        "else:\n",
        "    start_page = 1\n",
        "\n",
        "# Collect posts\n",
        "page = start_page\n",
        "limit_reached = False\n",
        "while len(posts) < 20000 and not limit_reached:\n",
        "    params = {\n",
        "        \"page\": page,\n",
        "        \"pagesize\": 100,\n",
        "        \"order\": \"desc\",\n",
        "        \"sort\": \"creation\",\n",
        "        \"tagged\": \"nlp\",\n",
        "        \"site\": \"stackoverflow\",\n",
        "        \"key\": API_KEY,\n",
        "        \"filter\": \"withbody\"\n",
        "    }\n",
        "    response = requests.get(BASE_URL, params=params)\n",
        "    data = response.json()\n",
        "\n",
        "    if \"error_id\" in data:  # Handle quota errors\n",
        "        print(f\"Error at page {page}: {data}\")\n",
        "        limit_reached = True\n",
        "        break\n",
        "    if \"items\" not in data:\n",
        "        print(f\"Error at page {page}: {data}\")\n",
        "        break\n",
        "    if not data[\"items\"]:\n",
        "        print(\"No more posts available.\")\n",
        "        break\n",
        "\n",
        "    for item in data[\"items\"]:\n",
        "        post_id = item[\"question_id\"]\n",
        "        accepted_answer_id = item.get(\"accepted_answer_id\", None)\n",
        "        accepted_answer_text = \"No accepted answer\"\n",
        "        more_answers_text = \"No additional answers\"\n",
        "\n",
        "        answers_url = f\"https://api.stackexchange.com/2.3/questions/{post_id}/answers\"\n",
        "        answers_params = {\n",
        "            \"site\": \"stackoverflow\",\n",
        "            \"key\": API_KEY,\n",
        "            \"filter\": \"withbody\",\n",
        "            \"order\": \"desc\",\n",
        "            \"sort\": \"votes\"\n",
        "        }\n",
        "        answers_response = requests.get(answers_url, params=answers_params)\n",
        "        answers_data = answers_response.json()\n",
        "\n",
        "        if \"error_id\" in answers_data:\n",
        "            print(f\"Error fetching answers for post {post_id}: {answers_data}\")\n",
        "            limit_reached = True\n",
        "            break\n",
        "        if \"items\" not in answers_data:\n",
        "            print(f\"Error fetching answers for post {post_id}: {answers_data}\")\n",
        "            continue\n",
        "\n",
        "        if answers_data[\"items\"]:\n",
        "            for answer in answers_data[\"items\"]:\n",
        "                answer_id = answer[\"answer_id\"]\n",
        "                if answer_id == accepted_answer_id:\n",
        "                    accepted_answer_text = answer[\"body\"]\n",
        "                elif more_answers_text == \"No additional answers\":\n",
        "                    more_answers_text = answer[\"body\"]\n",
        "                    break\n",
        "\n",
        "        posts.append({\n",
        "            \"title\": item[\"title\"],\n",
        "            \"description\": item[\"body\"],\n",
        "            \"tags\": \";\".join(item[\"tags\"]),\n",
        "            \"accepted_answer\": accepted_answer_text,\n",
        "            \"more_accepted_answers\": more_answers_text\n",
        "        })\n",
        "\n",
        "    print(f\"Collected page {page}, total posts: {len(posts)}\")\n",
        "    page += 1\n",
        "    time.sleep(1)\n",
        "\n",
        "    # Save after every page\n",
        "    initial_df = pd.DataFrame(posts, columns=[\"title\", \"description\", \"tags\", \"accepted_answer\", \"more_accepted_answers\"])\n",
        "    with pd.ExcelWriter(EXCEL_FILE, engine=\"xlsxwriter\") as writer:\n",
        "        initial_df.to_excel(writer, sheet_name=\"Raw_Data\", index=False)\n",
        "    print(f\"Saved page {page - 1} to {EXCEL_FILE}\")\n",
        "\n",
        "    if \"quota_remaining\" in data and data[\"quota_remaining\"] < 100:\n",
        "        print(f\"Quota nearly exhausted ({data['quota_remaining']} remaining), moving to preprocessing...\")\n",
        "        limit_reached = True\n",
        "        break\n",
        "\n",
        "# Final save\n",
        "initial_df = pd.DataFrame(posts, columns=[\"title\", \"description\", \"tags\", \"accepted_answer\", \"more_accepted_answers\"])\n",
        "with pd.ExcelWriter(EXCEL_FILE, engine=\"xlsxwriter\") as writer:\n",
        "    initial_df.to_excel(writer, sheet_name=\"Raw_Data\", index=False)\n",
        "print(f\"Collected {len(initial_df)} posts, saved to {EXCEL_FILE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Second Data Collection Run (Failed due to the IllegalCharacterError)"
      ],
      "metadata": {
        "id": "t6nrgAelQClY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "API_KEY = \"rl_kUCy4GWQMRSP2AHqViJ1AsvUU\"  # new API\n",
        "\n",
        "# Load existing data if file exists\n",
        "if os.path.exists(EXCEL_FILE):\n",
        "    existing_df = pd.read_excel(EXCEL_FILE, sheet_name=\"Raw_Data\")\n",
        "    posts = existing_df.to_dict(\"records\")\n",
        "    start_page = (len(posts) // 100) + 1  # Resume from next page\n",
        "    print(f\"Resuming from page {start_page}, {len(posts)} posts already collected\")\n",
        "else:\n",
        "    print(\"No existing file found, starting fresh.\")\n",
        "    posts = []\n",
        "    start_page = 1\n",
        "\n",
        "# Collect posts\n",
        "page = start_page\n",
        "limit_reached = False\n",
        "while len(posts) < 20000 and not limit_reached:\n",
        "    params = {\n",
        "        \"page\": page,\n",
        "        \"pagesize\": 100,\n",
        "        \"order\": \"desc\",\n",
        "        \"sort\": \"creation\",\n",
        "        \"tagged\": \"nlp\",\n",
        "        \"site\": \"stackoverflow\",\n",
        "        \"key\": API_KEY,\n",
        "        \"filter\": \"withbody\"\n",
        "    }\n",
        "    response = requests.get(BASE_URL, params=params)\n",
        "    data = response.json()\n",
        "\n",
        "    if \"error_id\" in data:\n",
        "        print(f\"Error at page {page}: {data}\")\n",
        "        limit_reached = True\n",
        "        break\n",
        "    if \"items\" not in data:\n",
        "        print(f\"Error at page {page}: {data}\")\n",
        "        break\n",
        "    if not data[\"items\"]:\n",
        "        print(\"No more posts available.\")\n",
        "        break\n",
        "\n",
        "    new_posts = []  # Collect new posts for this page\n",
        "    for item in data[\"items\"]:\n",
        "        post_id = item[\"question_id\"]\n",
        "        accepted_answer_id = item.get(\"accepted_answer_id\", None)\n",
        "        accepted_answer_text = \"No accepted answer\"\n",
        "        more_answers_text = \"No additional answers\"\n",
        "\n",
        "        answers_url = f\"https://api.stackexchange.com/2.3/questions/{post_id}/answers\"\n",
        "        answers_params = {\n",
        "            \"site\": \"stackoverflow\",\n",
        "            \"key\": API_KEY,\n",
        "            \"filter\": \"withbody\",\n",
        "            \"order\": \"desc\",\n",
        "            \"sort\": \"votes\"\n",
        "        }\n",
        "        answers_response = requests.get(answers_url, params=answers_params)\n",
        "        answers_data = answers_response.json()\n",
        "\n",
        "        if \"error_id\" in answers_data:\n",
        "            print(f\"Error fetching answers for post {post_id}: {answers_data}\")\n",
        "            limit_reached = True\n",
        "            break\n",
        "        if \"items\" not in answers_data:\n",
        "            print(f\"Error fetching answers for post {post_id}: {answers_data}\")\n",
        "            continue\n",
        "\n",
        "        if answers_data[\"items\"]:\n",
        "            for answer in answers_data[\"items\"]:\n",
        "                answer_id = answer[\"answer_id\"]\n",
        "                if answer_id == accepted_answer_id:\n",
        "                    accepted_answer_text = answer[\"body\"]\n",
        "                elif more_answers_text == \"No additional answers\":\n",
        "                    more_answers_text = answer[\"body\"]\n",
        "                    break\n",
        "\n",
        "        new_posts.append({\n",
        "            \"title\": item[\"title\"],\n",
        "            \"description\": item[\"body\"],\n",
        "            \"tags\": \";\".join(item[\"tags\"]),\n",
        "            \"accepted_answer\": accepted_answer_text,\n",
        "            \"more_accepted_answers\": more_answers_text\n",
        "        })\n",
        "\n",
        "    # Append new posts to existing list\n",
        "    posts.extend(new_posts)\n",
        "    print(f\"Collected page {page}, total posts: {len(posts)}\")\n",
        "    page += 1\n",
        "    time.sleep(1)\n",
        "\n",
        "    # Append to existing Excel file\n",
        "    new_df = pd.DataFrame(new_posts, columns=[\"title\", \"description\", \"tags\", \"accepted_answer\", \"more_accepted_answers\"])\n",
        "    if os.path.exists(EXCEL_FILE):\n",
        "        book = load_workbook(EXCEL_FILE)\n",
        "        with pd.ExcelWriter(EXCEL_FILE, engine=\"openpyxl\", mode=\"a\", if_sheet_exists=\"overlay\") as writer:\n",
        "            # Append new data below existing rows\n",
        "            start_row = len(existing_df) + 1  # Start after existing data (header + rows)\n",
        "            new_df.to_excel(writer, sheet_name=\"Raw_Data\", startrow=start_row, index=False, header=False)\n",
        "    else:\n",
        "        # Create new file if it doesn’t exist (first run)\n",
        "        with pd.ExcelWriter(EXCEL_FILE, engine=\"openpyxl\") as writer:\n",
        "            new_df.to_excel(writer, sheet_name=\"Raw_Data\", index=False)\n",
        "    print(f\"Saved page {page - 1} to {EXCEL_FILE}, appended {len(new_posts)} new posts\")\n",
        "\n",
        "    if \"quota_remaining\" in data and data[\"quota_remaining\"] < 100:\n",
        "        print(f\"Quota nearly exhausted ({data['quota_remaining']} remaining), moving to preprocessing...\")\n",
        "        limit_reached = True\n",
        "        break\n",
        "\n",
        "# Final save\n",
        "initial_df = pd.DataFrame(posts, columns=[\"title\", \"description\", \"tags\", \"accepted_answer\", \"more_accepted_answers\"])\n",
        "with pd.ExcelWriter(EXCEL_FILE, engine=\"openpyxl\") as writer:\n",
        "    initial_df.to_excel(writer, sheet_name=\"Raw_Data\", index=False)\n",
        "print(f\"Collected {len(initial_df)} posts, saved to {EXCEL_FILE}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hbbCugQyhlAd",
        "outputId": "29ab75df-826a-46f3-ac62-3f05d44d1038"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resuming from page 99, 9806 posts already collected\n",
            "Collected page 99, total posts: 9906\n",
            "Saved page 99 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 100, total posts: 10006\n",
            "Saved page 100 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 101, total posts: 10106\n",
            "Saved page 101 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 102, total posts: 10206\n",
            "Saved page 102 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 103, total posts: 10306\n",
            "Saved page 103 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 104, total posts: 10406\n",
            "Saved page 104 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 105, total posts: 10506\n",
            "Saved page 105 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 106, total posts: 10606\n",
            "Saved page 106 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 107, total posts: 10706\n",
            "Saved page 107 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 108, total posts: 10806\n",
            "Saved page 108 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 109, total posts: 10906\n",
            "Saved page 109 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 110, total posts: 11006\n",
            "Saved page 110 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 111, total posts: 11106\n",
            "Saved page 111 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 112, total posts: 11206\n",
            "Saved page 112 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 113, total posts: 11306\n",
            "Saved page 113 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 114, total posts: 11406\n",
            "Saved page 114 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 115, total posts: 11506\n",
            "Saved page 115 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 116, total posts: 11606\n",
            "Saved page 116 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 117, total posts: 11706\n",
            "Saved page 117 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 118, total posts: 11806\n",
            "Saved page 118 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 119, total posts: 11906\n",
            "Saved page 119 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 120, total posts: 12006\n",
            "Saved page 120 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 121, total posts: 12106\n",
            "Saved page 121 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 122, total posts: 12205\n",
            "Saved page 122 to /content/nlp_stackoverflow_dataset.xlsx, appended 99 new posts\n",
            "Collected page 123, total posts: 12305\n",
            "Saved page 123 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 124, total posts: 12405\n",
            "Saved page 124 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 125, total posts: 12505\n",
            "Saved page 125 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 126, total posts: 12605\n",
            "Saved page 126 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 127, total posts: 12705\n",
            "Saved page 127 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 128, total posts: 12805\n",
            "Saved page 128 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 129, total posts: 12905\n",
            "Saved page 129 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 130, total posts: 13005\n",
            "Saved page 130 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 131, total posts: 13104\n",
            "Saved page 131 to /content/nlp_stackoverflow_dataset.xlsx, appended 99 new posts\n",
            "Collected page 132, total posts: 13204\n",
            "Saved page 132 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 133, total posts: 13303\n",
            "Saved page 133 to /content/nlp_stackoverflow_dataset.xlsx, appended 99 new posts\n",
            "Collected page 134, total posts: 13403\n",
            "Saved page 134 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 135, total posts: 13503\n",
            "Saved page 135 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 136, total posts: 13602\n",
            "Saved page 136 to /content/nlp_stackoverflow_dataset.xlsx, appended 99 new posts\n",
            "Collected page 137, total posts: 13702\n",
            "Saved page 137 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 138, total posts: 13802\n",
            "Saved page 138 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 139, total posts: 13901\n",
            "Saved page 139 to /content/nlp_stackoverflow_dataset.xlsx, appended 99 new posts\n",
            "Collected page 140, total posts: 14001\n",
            "Saved page 140 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 141, total posts: 14101\n",
            "Saved page 141 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 142, total posts: 14201\n",
            "Saved page 142 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 143, total posts: 14301\n",
            "Saved page 143 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 144, total posts: 14400\n",
            "Saved page 144 to /content/nlp_stackoverflow_dataset.xlsx, appended 99 new posts\n",
            "Collected page 145, total posts: 14500\n",
            "Saved page 145 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 146, total posts: 14600\n",
            "Saved page 146 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 147, total posts: 14700\n",
            "Saved page 147 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 148, total posts: 14800\n",
            "Saved page 148 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 149, total posts: 14899\n",
            "Saved page 149 to /content/nlp_stackoverflow_dataset.xlsx, appended 99 new posts\n",
            "Collected page 150, total posts: 14999\n",
            "Saved page 150 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 151, total posts: 15099\n",
            "Saved page 151 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 152, total posts: 15199\n",
            "Saved page 152 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 153, total posts: 15299\n",
            "Saved page 153 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 154, total posts: 15399\n",
            "Saved page 154 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 155, total posts: 15499\n",
            "Saved page 155 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 156, total posts: 15599\n",
            "Saved page 156 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 157, total posts: 15699\n",
            "Saved page 157 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 158, total posts: 15799\n",
            "Saved page 158 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 159, total posts: 15899\n",
            "Saved page 159 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 160, total posts: 15999\n",
            "Saved page 160 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 161, total posts: 16099\n",
            "Saved page 161 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 162, total posts: 16198\n",
            "Saved page 162 to /content/nlp_stackoverflow_dataset.xlsx, appended 99 new posts\n",
            "Collected page 163, total posts: 16298\n",
            "Saved page 163 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 164, total posts: 16398\n",
            "Saved page 164 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 165, total posts: 16498\n",
            "Saved page 165 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 166, total posts: 16598\n",
            "Saved page 166 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 167, total posts: 16696\n",
            "Saved page 167 to /content/nlp_stackoverflow_dataset.xlsx, appended 98 new posts\n",
            "Collected page 168, total posts: 16796\n",
            "Saved page 168 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 169, total posts: 16896\n",
            "Saved page 169 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 170, total posts: 16996\n",
            "Saved page 170 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 171, total posts: 17096\n",
            "Saved page 171 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 172, total posts: 17196\n",
            "Saved page 172 to /content/nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 173, total posts: 17295\n",
            "Saved page 173 to /content/nlp_stackoverflow_dataset.xlsx, appended 99 new posts\n",
            "Collected page 174, total posts: 17395\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IllegalCharacterError",
          "evalue": "<p>\"Another systemic problem with Naive Bayes is that\nfeatures are assumed to be independent. As a result,\neven when words are dependent, each word contributes\nevidence individually. Thus the magnitude of\nthe weights for classes with strong word dependencies\nis larger than for classes with weak word dependencies.\nTo keep classes with more dependencies from dominating,\nwe normalize the classi\fcation weights.\" (<a href=\"http://www.aaai.org/Papers/ICML/2003/ICML03-081.pdf\" rel=\"nofollow\">Reference</a> )</p>\n\n<p>What does this exactly mean? Is there any example that explains it better?</p>\n cannot be used in worksheets.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIllegalCharacterError\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b84338c0bb2e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;31m# Append new data below existing rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mstart_row\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexisting_df\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m  \u001b[0;31m# Start after existing data (header + rows)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0mnew_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msheet_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Raw_Data\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstartrow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_row\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;31m# Create new file if it doesn’t exist (first run)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m                 )\n\u001b[0;32m--> 333\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_excel\u001b[0;34m(self, excel_writer, sheet_name, na_rep, float_format, columns, header, index, index_label, startrow, startcol, engine, merge_cells, inf_rep, freeze_panes, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m   2415\u001b[0m             \u001b[0minf_rep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minf_rep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2416\u001b[0m         )\n\u001b[0;32m-> 2417\u001b[0;31m         formatter.write(\n\u001b[0m\u001b[1;32m   2418\u001b[0m             \u001b[0mexcel_writer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m             \u001b[0msheet_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msheet_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/formats/excel.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, writer, sheet_name, startrow, startcol, freeze_panes, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             writer._write_cells(\n\u001b[0m\u001b[1;32m    953\u001b[0m                 \u001b[0mformatted_cells\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m                 \u001b[0msheet_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_openpyxl.py\u001b[0m in \u001b[0;36m_write_cells\u001b[0;34m(self, cells, sheet_name, startrow, startcol, freeze_panes)\u001b[0m\n\u001b[1;32m    488\u001b[0m                 \u001b[0mrow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstartrow\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrow\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstartcol\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m             )\n\u001b[0;32m--> 490\u001b[0;31m             \u001b[0mxcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value_with_fmt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    491\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m                 \u001b[0mxcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openpyxl/cell/cell.py\u001b[0m in \u001b[0;36mvalue\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;34m\"\"\"Set the value and infer type and display options.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bind_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openpyxl/cell/cell.py\u001b[0m in \u001b[0;36m_bind_value\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mdt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"s\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCellRichText\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'f'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openpyxl/cell/cell.py\u001b[0m in \u001b[0;36mcheck_string\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m32767\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mILLEGAL_CHARACTERS_RE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIllegalCharacterError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{value} cannot be used in worksheets.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIllegalCharacterError\u001b[0m: <p>\"Another systemic problem with Naive Bayes is that\nfeatures are assumed to be independent. As a result,\neven when words are dependent, each word contributes\nevidence individually. Thus the magnitude of\nthe weights for classes with strong word dependencies\nis larger than for classes with weak word dependencies.\nTo keep classes with more dependencies from dominating,\nwe normalize the classi\fcation weights.\" (<a href=\"http://www.aaai.org/Papers/ICML/2003/ICML03-081.pdf\" rel=\"nofollow\">Reference</a> )</p>\n\n<p>What does this exactly mean? Is there any example that explains it better?</p>\n cannot be used in worksheets."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Third Data Collection Run (Successfully completed but excel sheet missing entries from page 99 to 174)"
      ],
      "metadata": {
        "id": "AGHS1jauQaLh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "API_KEY = \"rl_kUCy4GWQMRSP2AHqViJ1AsvUU\"  # new API key\n",
        "\n",
        "# Function to remove illegal characters\n",
        "def clean_text(text):\n",
        "    if isinstance(text, str):\n",
        "        # Remove control characters and other illegal characters\n",
        "        text = re.sub(r'[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F\\x7F-\\x9F]', '', text)\n",
        "    return text\n",
        "\n",
        "# Load existing data and determine start page\n",
        "if os.path.exists(EXCEL_FILE):\n",
        "    existing_df = pd.read_excel(EXCEL_FILE, sheet_name=\"Raw_Data\")\n",
        "    posts = existing_df.to_dict(\"records\")\n",
        "    start_page = (len(posts) // 100) + 1\n",
        "    print(f\"Resuming from page {start_page}, {len(posts)} posts already collected\")\n",
        "else:\n",
        "    print(\"No existing file found, starting fresh.\")\n",
        "    posts = []\n",
        "    start_page = 1\n",
        "\n",
        "# Collect posts\n",
        "page = start_page\n",
        "limit_reached = False\n",
        "while len(posts) < 20000 and not limit_reached:\n",
        "    params = {\n",
        "        \"page\": page,\n",
        "        \"pagesize\": 100,\n",
        "        \"order\": \"desc\",\n",
        "        \"sort\": \"creation\",\n",
        "        \"tagged\": \"nlp\",\n",
        "        \"site\": \"stackoverflow\",\n",
        "        \"key\": API_KEY,\n",
        "        \"filter\": \"withbody\"\n",
        "    }\n",
        "    response = requests.get(BASE_URL, params=params)\n",
        "    data = response.json()\n",
        "\n",
        "    if \"error_id\" in data:\n",
        "        print(f\"Error at page {page}: {data}\")\n",
        "        limit_reached = True\n",
        "        break\n",
        "    if \"items\" not in data:\n",
        "        print(f\"Error at page {page}: {data}\")\n",
        "        break\n",
        "    if not data[\"items\"]:\n",
        "        print(\"No more posts available.\")\n",
        "        break\n",
        "\n",
        "    new_posts = []\n",
        "    for item in data[\"items\"]:\n",
        "        try:\n",
        "            post_id = item[\"question_id\"]\n",
        "            accepted_answer_id = item.get(\"accepted_answer_id\", None)\n",
        "            accepted_answer_text = \"No accepted answer\"\n",
        "            more_answers_text = \"No additional answers\"\n",
        "\n",
        "            answers_url = f\"https://api.stackexchange.com/2.3/questions/{post_id}/answers\"\n",
        "            answers_params = {\n",
        "                \"site\": \"stackoverflow\",\n",
        "                \"key\": API_KEY,\n",
        "                \"filter\": \"withbody\",\n",
        "                \"order\": \"desc\",\n",
        "                \"sort\": \"votes\"\n",
        "            }\n",
        "            answers_response = requests.get(answers_url, params=answers_params)\n",
        "            answers_data = answers_response.json()\n",
        "\n",
        "            if \"error_id\" in answers_data:\n",
        "                print(f\"Error fetching answers for post {post_id}: {answers_data}\")\n",
        "                limit_reached = True\n",
        "                break\n",
        "            if \"items\" not in answers_data:\n",
        "                print(f\"Error fetching answers for post {post_id}: {answers_data}\")\n",
        "                continue\n",
        "\n",
        "            if answers_data[\"items\"]:\n",
        "                for answer in answers_data[\"items\"]:\n",
        "                    answer_id = answer[\"answer_id\"]\n",
        "                    if answer_id == accepted_answer_id:\n",
        "                        accepted_answer_text = answer[\"body\"]\n",
        "                    elif more_answers_text == \"No additional answers\":\n",
        "                        more_answers_text = answer[\"body\"]\n",
        "                        break\n",
        "\n",
        "            new_posts.append({\n",
        "                \"title\": clean_text(item[\"title\"]),\n",
        "                \"description\": clean_text(item[\"body\"]),\n",
        "                \"tags\": clean_text(\";\".join(item[\"tags\"])),\n",
        "                \"accepted_answer\": clean_text(accepted_answer_text),\n",
        "                \"more_accepted_answers\": clean_text(more_answers_text)\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping post {post_id} due to error: {e}\")\n",
        "            continue\n",
        "\n",
        "    posts.extend(new_posts)\n",
        "    print(f\"Collected page {page}, total posts: {len(posts)}\")\n",
        "    page += 1\n",
        "    time.sleep(1)\n",
        "\n",
        "    # Append to existing Excel file\n",
        "    new_df = pd.DataFrame(new_posts, columns=[\"title\", \"description\", \"tags\", \"accepted_answer\", \"more_accepted_answers\"])\n",
        "    if os.path.exists(EXCEL_FILE):\n",
        "        book = load_workbook(EXCEL_FILE)\n",
        "        with pd.ExcelWriter(EXCEL_FILE, engine=\"openpyxl\", mode=\"a\", if_sheet_exists=\"overlay\") as writer:\n",
        "            start_row = len(existing_df) + 1  # Append after existing data\n",
        "            new_df.to_excel(writer, sheet_name=\"Raw_Data\", startrow=start_row, index=False, header=False)\n",
        "    else:\n",
        "        with pd.ExcelWriter(EXCEL_FILE, engine=\"openpyxl\") as writer:\n",
        "            new_df.to_excel(writer, sheet_name=\"Raw_Data\", index=False)\n",
        "    print(f\"Saved page {page - 1} to {EXCEL_FILE}, appended {len(new_posts)} new posts\")\n",
        "\n",
        "    if \"quota_remaining\" in data and data[\"quota_remaining\"] < 100:\n",
        "        print(f\"Quota nearly exhausted ({data['quota_remaining']} remaining), moving to preprocessing...\")\n",
        "        limit_reached = True\n",
        "        break\n",
        "\n",
        "# Final save\n",
        "initial_df = pd.DataFrame(posts, columns=[\"title\", \"description\", \"tags\", \"accepted_answer\", \"more_accepted_answers\"])\n",
        "with pd.ExcelWriter(EXCEL_FILE, engine=\"openpyxl\") as writer:\n",
        "    initial_df.to_excel(writer, sheet_name=\"Raw_Data\", index=False)\n",
        "print(f\"Collected {len(initial_df)} posts, saved to {EXCEL_FILE}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "miWbhVpKpeFl",
        "outputId": "64fd29f7-1725-4399-a67d-a6974e2e34e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 10006 posts from existing file\n",
            "Starting collection from page 174\n",
            "Collected page 174, total posts: 10106\n",
            "Saved page 174 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 175, total posts: 10206\n",
            "Saved page 175 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 176, total posts: 10306\n",
            "Saved page 176 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 177, total posts: 10406\n",
            "Saved page 177 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 178, total posts: 10506\n",
            "Saved page 178 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 179, total posts: 10606\n",
            "Saved page 179 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 180, total posts: 10706\n",
            "Saved page 180 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 181, total posts: 10804\n",
            "Saved page 181 to nlp_stackoverflow_dataset.xlsx, appended 98 new posts\n",
            "Collected page 182, total posts: 10901\n",
            "Saved page 182 to nlp_stackoverflow_dataset.xlsx, appended 97 new posts\n",
            "Collected page 183, total posts: 11001\n",
            "Saved page 183 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 184, total posts: 11101\n",
            "Saved page 184 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 185, total posts: 11201\n",
            "Saved page 185 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 186, total posts: 11301\n",
            "Saved page 186 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 187, total posts: 11401\n",
            "Saved page 187 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 188, total posts: 11500\n",
            "Saved page 188 to nlp_stackoverflow_dataset.xlsx, appended 99 new posts\n",
            "Collected page 189, total posts: 11599\n",
            "Saved page 189 to nlp_stackoverflow_dataset.xlsx, appended 99 new posts\n",
            "Collected page 190, total posts: 11699\n",
            "Saved page 190 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 191, total posts: 11799\n",
            "Saved page 191 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 192, total posts: 11899\n",
            "Saved page 192 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 193, total posts: 11998\n",
            "Saved page 193 to nlp_stackoverflow_dataset.xlsx, appended 99 new posts\n",
            "Collected page 194, total posts: 12098\n",
            "Saved page 194 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 195, total posts: 12198\n",
            "Saved page 195 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 196, total posts: 12298\n",
            "Saved page 196 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 197, total posts: 12398\n",
            "Saved page 197 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 198, total posts: 12497\n",
            "Saved page 198 to nlp_stackoverflow_dataset.xlsx, appended 99 new posts\n",
            "Collected page 199, total posts: 12597\n",
            "Saved page 199 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 200, total posts: 12697\n",
            "Saved page 200 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 201, total posts: 12796\n",
            "Saved page 201 to nlp_stackoverflow_dataset.xlsx, appended 99 new posts\n",
            "Collected page 202, total posts: 12896\n",
            "Saved page 202 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 203, total posts: 12996\n",
            "Saved page 203 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 204, total posts: 13096\n",
            "Saved page 204 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 205, total posts: 13196\n",
            "Saved page 205 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 206, total posts: 13296\n",
            "Saved page 206 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 207, total posts: 13395\n",
            "Saved page 207 to nlp_stackoverflow_dataset.xlsx, appended 99 new posts\n",
            "Collected page 208, total posts: 13495\n",
            "Saved page 208 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 209, total posts: 13595\n",
            "Saved page 209 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 210, total posts: 13621\n",
            "Saved page 210 to nlp_stackoverflow_dataset.xlsx, appended 26 new posts\n",
            "No more posts available.\n",
            "Collected 13621 posts, saved to nlp_stackoverflow_dataset.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fourth Data Collection Run (successfully completed)"
      ],
      "metadata": {
        "id": "hmFWNymGQ2wy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "API_KEY = \"rl_kUCy4GWQMRSP2AHqViJ1AsvUU\"  # new API key\n",
        "\n",
        "# Load existing data\n",
        "if os.path.exists(EXCEL_FILE):\n",
        "    existing_df = pd.read_excel(EXCEL_FILE, sheet_name=\"Raw_Data\")\n",
        "    posts = existing_df.to_dict(\"records\")\n",
        "    print(f\"Loaded {len(posts)} posts from existing file\")\n",
        "else:\n",
        "    print(\"No existing file found, starting fresh.\")\n",
        "    posts = []\n",
        "\n",
        "# Set explicit page range: 99 to 174\n",
        "start_page = 99\n",
        "end_page = 174\n",
        "print(f\"Collecting from page {start_page} to {end_page}\")\n",
        "\n",
        "# Collect posts\n",
        "page = start_page\n",
        "limit_reached = False\n",
        "while page <= end_page and len(posts) < 50000 and not limit_reached:\n",
        "    params = {\n",
        "        \"page\": page,\n",
        "        \"pagesize\": 100,\n",
        "        \"order\": \"desc\",\n",
        "        \"sort\": \"creation\",\n",
        "        \"tagged\": \"nlp\",\n",
        "        \"site\": \"stackoverflow\",\n",
        "        \"key\": API_KEY,\n",
        "        \"filter\": \"withbody\"\n",
        "    }\n",
        "    response = requests.get(BASE_URL, params=params)\n",
        "    data = response.json()\n",
        "\n",
        "    if \"error_id\" in data:\n",
        "        print(f\"Error at page {page}: {data}\")\n",
        "        limit_reached = True\n",
        "        break\n",
        "    if \"items\" not in data:\n",
        "        print(f\"Error at page {page}: {data}\")\n",
        "        break\n",
        "    if not data[\"items\"]:\n",
        "        print(f\"No more posts available at page {page}.\")\n",
        "        break\n",
        "\n",
        "    new_posts = []\n",
        "    for item in data[\"items\"]:\n",
        "        try:\n",
        "            post_id = item[\"question_id\"]\n",
        "            accepted_answer_id = item.get(\"accepted_answer_id\", None)\n",
        "            accepted_answer_text = \"No accepted answer\"\n",
        "            more_answers_text = \"No additional answers\"\n",
        "\n",
        "            answers_url = f\"https://api.stackexchange.com/2.3/questions/{post_id}/answers\"\n",
        "            answers_params = {\n",
        "                \"site\": \"stackoverflow\",\n",
        "                \"key\": API_KEY,\n",
        "                \"filter\": \"withbody\",\n",
        "                \"order\": \"desc\",\n",
        "                \"sort\": \"votes\"\n",
        "            }\n",
        "            answers_response = requests.get(answers_url, params=answers_params)\n",
        "            answers_data = answers_response.json()\n",
        "\n",
        "            if \"error_id\" in answers_data:\n",
        "                print(f\"Error fetching answers for post {post_id}: {answers_data}\")\n",
        "                limit_reached = True\n",
        "                break\n",
        "            if \"items\" not in answers_data:\n",
        "                print(f\"Error fetching answers for post {post_id}: {answers_data}\")\n",
        "                continue\n",
        "\n",
        "            if answers_data[\"items\"]:\n",
        "                for answer in answers_data[\"items\"]:\n",
        "                    answer_id = answer[\"answer_id\"]\n",
        "                    if answer_id == accepted_answer_id:\n",
        "                        accepted_answer_text = answer[\"body\"]\n",
        "                    elif more_answers_text == \"No additional answers\":\n",
        "                        more_answers_text = answer[\"body\"]\n",
        "                        break\n",
        "\n",
        "            new_posts.append({\n",
        "                \"title\": clean_text(item[\"title\"]),\n",
        "                \"description\": clean_text(item[\"body\"]),\n",
        "                \"tags\": clean_text(\";\".join(item[\"tags\"])),\n",
        "                \"accepted_answer\": clean_text(accepted_answer_text),\n",
        "                \"more_accepted_answers\": clean_text(more_answers_text)\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping post {post_id} due to error: {e}\")\n",
        "            continue\n",
        "\n",
        "    posts.extend(new_posts)\n",
        "    print(f\"Collected page {page}, total posts: {len(posts)}, posts this page: {len(new_posts)}\")\n",
        "    page += 1\n",
        "    time.sleep(1)\n",
        "\n",
        "    # Append to existing Excel file\n",
        "    new_df = pd.DataFrame(new_posts, columns=[\"title\", \"description\", \"tags\", \"accepted_answer\", \"more_accepted_answers\"])\n",
        "    if os.path.exists(EXCEL_FILE):\n",
        "        book = load_workbook(EXCEL_FILE)\n",
        "        with pd.ExcelWriter(EXCEL_FILE, engine=\"openpyxl\", mode=\"a\", if_sheet_exists=\"overlay\") as writer:\n",
        "            start_row = len(existing_df) + 1  # Append after existing data\n",
        "            new_df.to_excel(writer, sheet_name=\"Raw_Data\", startrow=start_row, index=False, header=False)\n",
        "    else:\n",
        "        with pd.ExcelWriter(EXCEL_FILE, engine=\"openpyxl\") as writer:\n",
        "            new_df.to_excel(writer, sheet_name=\"Raw_Data\", index=False)\n",
        "    print(f\"Saved page {page - 1} to {EXCEL_FILE}, appended {len(new_posts)} new posts\")\n",
        "\n",
        "    if \"quota_remaining\" in data and data[\"quota_remaining\"] < 100:\n",
        "        print(f\"Quota nearly exhausted ({data['quota_remaining']} remaining), stopping...\")\n",
        "        limit_reached = True\n",
        "        break\n",
        "\n",
        "# Final save\n",
        "initial_df = pd.DataFrame(posts, columns=[\"title\", \"description\", \"tags\", \"accepted_answer\", \"more_accepted_answers\"])\n",
        "with pd.ExcelWriter(EXCEL_FILE, engine=\"openpyxl\") as writer:\n",
        "    initial_df.to_excel(writer, sheet_name=\"Raw_Data\", index=False)\n",
        "print(f\"Collected {len(initial_df)} posts, saved to {EXCEL_FILE}\")"
      ],
      "metadata": {
        "id": "Poxb0PRot2VL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c63de8a-b988-4782-d6ed-99a29f1fdd5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 13657 posts from existing file\n",
            "Collecting from page 99 to 174\n",
            "Collected page 99, total posts: 13757, posts this page: 100\n",
            "Saved page 99 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 100, total posts: 13857, posts this page: 100\n",
            "Saved page 100 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 101, total posts: 13957, posts this page: 100\n",
            "Saved page 101 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 102, total posts: 14057, posts this page: 100\n",
            "Saved page 102 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 103, total posts: 14157, posts this page: 100\n",
            "Saved page 103 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 104, total posts: 14257, posts this page: 100\n",
            "Saved page 104 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 105, total posts: 14357, posts this page: 100\n",
            "Saved page 105 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 106, total posts: 14457, posts this page: 100\n",
            "Saved page 106 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 107, total posts: 14557, posts this page: 100\n",
            "Saved page 107 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 108, total posts: 14657, posts this page: 100\n",
            "Saved page 108 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 109, total posts: 14757, posts this page: 100\n",
            "Saved page 109 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 110, total posts: 14857, posts this page: 100\n",
            "Saved page 110 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 111, total posts: 14957, posts this page: 100\n",
            "Saved page 111 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 112, total posts: 15057, posts this page: 100\n",
            "Saved page 112 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 113, total posts: 15157, posts this page: 100\n",
            "Saved page 113 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 114, total posts: 15257, posts this page: 100\n",
            "Saved page 114 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 115, total posts: 15357, posts this page: 100\n",
            "Saved page 115 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 116, total posts: 15457, posts this page: 100\n",
            "Saved page 116 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 117, total posts: 15557, posts this page: 100\n",
            "Saved page 117 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 118, total posts: 15657, posts this page: 100\n",
            "Saved page 118 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 119, total posts: 15757, posts this page: 100\n",
            "Saved page 119 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 120, total posts: 15857, posts this page: 100\n",
            "Saved page 120 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 121, total posts: 15957, posts this page: 100\n",
            "Saved page 121 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 122, total posts: 16056, posts this page: 99\n",
            "Saved page 122 to nlp_stackoverflow_dataset.xlsx, appended 99 new posts\n",
            "Collected page 123, total posts: 16156, posts this page: 100\n",
            "Saved page 123 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 124, total posts: 16256, posts this page: 100\n",
            "Saved page 124 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 125, total posts: 16356, posts this page: 100\n",
            "Saved page 125 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 126, total posts: 16456, posts this page: 100\n",
            "Saved page 126 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 127, total posts: 16556, posts this page: 100\n",
            "Saved page 127 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 128, total posts: 16656, posts this page: 100\n",
            "Saved page 128 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 129, total posts: 16756, posts this page: 100\n",
            "Saved page 129 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 130, total posts: 16856, posts this page: 100\n",
            "Saved page 130 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 131, total posts: 16955, posts this page: 99\n",
            "Saved page 131 to nlp_stackoverflow_dataset.xlsx, appended 99 new posts\n",
            "Collected page 132, total posts: 17055, posts this page: 100\n",
            "Saved page 132 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 133, total posts: 17154, posts this page: 99\n",
            "Saved page 133 to nlp_stackoverflow_dataset.xlsx, appended 99 new posts\n",
            "Collected page 134, total posts: 17254, posts this page: 100\n",
            "Saved page 134 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 135, total posts: 17354, posts this page: 100\n",
            "Saved page 135 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 136, total posts: 17453, posts this page: 99\n",
            "Saved page 136 to nlp_stackoverflow_dataset.xlsx, appended 99 new posts\n",
            "Collected page 137, total posts: 17553, posts this page: 100\n",
            "Saved page 137 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 138, total posts: 17653, posts this page: 100\n",
            "Saved page 138 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 139, total posts: 17752, posts this page: 99\n",
            "Saved page 139 to nlp_stackoverflow_dataset.xlsx, appended 99 new posts\n",
            "Collected page 140, total posts: 17852, posts this page: 100\n",
            "Saved page 140 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 141, total posts: 17952, posts this page: 100\n",
            "Saved page 141 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 142, total posts: 18052, posts this page: 100\n",
            "Saved page 142 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 143, total posts: 18152, posts this page: 100\n",
            "Saved page 143 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 144, total posts: 18251, posts this page: 99\n",
            "Saved page 144 to nlp_stackoverflow_dataset.xlsx, appended 99 new posts\n",
            "Collected page 145, total posts: 18351, posts this page: 100\n",
            "Saved page 145 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 146, total posts: 18451, posts this page: 100\n",
            "Saved page 146 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 147, total posts: 18551, posts this page: 100\n",
            "Saved page 147 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 148, total posts: 18651, posts this page: 100\n",
            "Saved page 148 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 149, total posts: 18750, posts this page: 99\n",
            "Saved page 149 to nlp_stackoverflow_dataset.xlsx, appended 99 new posts\n",
            "Collected page 150, total posts: 18850, posts this page: 100\n",
            "Saved page 150 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 151, total posts: 18950, posts this page: 100\n",
            "Saved page 151 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 152, total posts: 19050, posts this page: 100\n",
            "Saved page 152 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 153, total posts: 19150, posts this page: 100\n",
            "Saved page 153 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 154, total posts: 19250, posts this page: 100\n",
            "Saved page 154 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 155, total posts: 19350, posts this page: 100\n",
            "Saved page 155 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 156, total posts: 19450, posts this page: 100\n",
            "Saved page 156 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 157, total posts: 19550, posts this page: 100\n",
            "Saved page 157 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 158, total posts: 19650, posts this page: 100\n",
            "Saved page 158 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 159, total posts: 19750, posts this page: 100\n",
            "Saved page 159 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 160, total posts: 19850, posts this page: 100\n",
            "Saved page 160 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 161, total posts: 19950, posts this page: 100\n",
            "Saved page 161 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 162, total posts: 20049, posts this page: 99\n",
            "Saved page 162 to nlp_stackoverflow_dataset.xlsx, appended 99 new posts\n",
            "Collected page 163, total posts: 20149, posts this page: 100\n",
            "Saved page 163 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 164, total posts: 20249, posts this page: 100\n",
            "Saved page 164 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 165, total posts: 20349, posts this page: 100\n",
            "Saved page 165 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 166, total posts: 20449, posts this page: 100\n",
            "Saved page 166 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 167, total posts: 20547, posts this page: 98\n",
            "Saved page 167 to nlp_stackoverflow_dataset.xlsx, appended 98 new posts\n",
            "Collected page 168, total posts: 20647, posts this page: 100\n",
            "Saved page 168 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 169, total posts: 20747, posts this page: 100\n",
            "Saved page 169 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 170, total posts: 20847, posts this page: 100\n",
            "Saved page 170 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 171, total posts: 20947, posts this page: 100\n",
            "Saved page 171 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 172, total posts: 21047, posts this page: 100\n",
            "Saved page 172 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected page 173, total posts: 21146, posts this page: 99\n",
            "Saved page 173 to nlp_stackoverflow_dataset.xlsx, appended 99 new posts\n",
            "Collected page 174, total posts: 21246, posts this page: 100\n",
            "Saved page 174 to nlp_stackoverflow_dataset.xlsx, appended 100 new posts\n",
            "Collected 21246 posts, saved to nlp_stackoverflow_dataset.xlsx\n"
          ]
        }
      ]
    }
  ]
}